{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rODb9vHvIEbp"
   },
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "U4KmHBd2cdx9"
   },
   "outputs": [],
   "source": [
    "# Add as many imports as you need.\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNMMoUiUIW3L"
   },
   "source": [
    "# Laboratory Exercise - Run Mode (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rAh_91OIjeS"
   },
   "source": [
    "## Introduction\n",
    "This laboratory assignment's primary objective is to fine-tune a pre-trained language model for binary classification on a dataset consisting of wine reviews. The dataset contains two attributes: **description** and **points**. The description is a brief text describing the wine and the points represent a quality metric ranging from 1 to 100. If some wine has at least 90 points it is considered **exceptional**. Your task involves predicting if some wine is **exceptional** based on its review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBYI-EypaWom"
   },
   "source": [
    "## The Wine Reviews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCm1qm1mZwMr"
   },
   "source": [
    "Load the dataset using the `datasets` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KMOn4fgcZn8s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['description', 'points'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here. Add as many boxes as you need.\n",
    "dataset = load_dataset('csv',data_files={'../datasets/wine-reviews.csv'})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZceBEFuiGpI"
   },
   "source": [
    "## Target Extraction\n",
    "Extract the target **exceptional** for each wine review. If some wine has at least 90 points it is considered **exceptional**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Duz8bzcRKSC7"
   },
   "outputs": [],
   "source": [
    "# Write your code here. Add as many boxes as you n\n",
    "def modify_points(example):\n",
    "    if example['points'] >=90:\n",
    "        example['points'] = 1\n",
    "    else:\n",
    "        example['points'] = 0\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'] = dataset['train'].map(modify_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': [\"Translucent in color, silky in the mouth, this Mendocino-grown Pinot shows ripe, forward flavors and crisp acidity. You'll find pie-filling cherries and rhubarb, with a cola and root beer soda finish.\",\n",
       "  'On the palate, this wine is rich and complex, with flavors of dried cherry, cola, licorice, red currant and sandalwood that finish bone dry. It has lots of acidity and some sandpaper-like tannins. This should begin to develop bottle-age notes after 2016.',\n",
       "  \"The producer blends 57% Chardonnay from the Maldonado Vineyard with 39% Sauvignon Blanc and 4% Viognier for this fresh, floral white. On the palate, apple and lemon flavors are buttressed by a core of laser-like acidity. A fresh pear note lasts long into the finish, proving the wine's ability to be both creamy and crisp.\",\n",
       "  \"Pure Baga in all its glory, packed with dry and dark tannins and a brooding character that is mitigated by the rich dark plum and blackberry jelly flavors. It's a wine that is probably at its peak, still with its tannins and structure although now with rich fruits and intense final acidity. Drink now and until 2022.\",\n",
       "  'Think of SubsÃ­dio as a contribution rather than as a subsidy, says the producerâ€”in this case a contribution to a meal. The wine is soft and fully ripe. It has generous tannins and black fruits. The rich wine, with its juicy aftertaste, is ready to drink.'],\n",
       " 'points': [0, 1, 1, 1, 0]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tls69_PrbJKW"
   },
   "source": [
    "## Dataset Splitting\n",
    "Partition the dataset into training and testing sets with an 80:20 ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PjGGGMxebeoB"
   },
   "outputs": [],
   "source": [
    "# Write your code here. Add as many boxes as you need.\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.2,seed=42)\n",
    "split_dataset = DatasetDict({\n",
    "    'train': split_dataset['train'],\n",
    "    'test': split_dataset['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLnr8v_dWO1i"
   },
   "source": [
    "## Tokenization\n",
    "Tokenize the texts using the `AutoTokenizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "esakOh8iWYEN"
   },
   "outputs": [],
   "source": [
    "# Write your code here. Add as many boxes as you need.\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['description'], truncation=True, padding=\"max_length\", max_length=15\n",
    "    )\n",
    "    tokenized['labels'] = examples['points']  # Ensure labels are included\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ae7d4fa52b413e9f85114b20811f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad493ab2bb242f789ef282e046f3617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = split_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['description']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['points', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['points', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIkAR1Hibiwr"
   },
   "source": [
    "## Fine-tuning a Pre-trained Language Model for Classification\n",
    "Fine-tune a pre-trained language model for classification on the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWn1pafKbnxH"
   },
   "source": [
    "Define the model using the `AutoModelForSequenceClassification` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IXFIrQthbnkb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Write your code here. Add as many boxes as you need.\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuQCSxZbYJOq"
   },
   "source": [
    "Define the traning parameters using the `TrainingArguments` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "p3sVs-L1YJHu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/miniconda3/envs/VNP/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Write your code here. Add as many boxes as you need.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./trainer',\n",
    "    learning_rate=1e-5,\n",
    "    metric_for_best_model='f1',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy='epoch',\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVvRS4L1YKHp"
   },
   "source": [
    "Define the training using the `Trainer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "O7cyG3PhYJ_M"
   },
   "outputs": [],
   "source": [
    "# Write your code here. Add as many boxes as you need.\n",
    "trainer = Trainer(\n",
    "    model=model,  # Ensure the model matches the task (e.g., binary classification)\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=tokenized_dataset['train'],  # Training dataset\n",
    "    eval_dataset=tokenized_dataset['test'],  # Evaluation dataset\n",
    "    processing_class=tokenizer,  # Tokenizer for compatibility\n",
    "    compute_metrics=compute_metrics  # Metrics calculation function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFZr21HxYkko"
   },
   "source": [
    "Fine-tune (train) the pre-trained lanugage model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "U9Dz0P11Ykeh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 06:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.612400</td>\n",
       "      <td>0.600467</td>\n",
       "      <td>0.670500</td>\n",
       "      <td>0.671017</td>\n",
       "      <td>0.670500</td>\n",
       "      <td>0.670251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.6268985290527344, metrics={'train_runtime': 396.9863, 'train_samples_per_second': 20.152, 'train_steps_per_second': 2.519, 'total_flos': 31047046560000.0, 'train_loss': 0.6268985290527344, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here. Add as many boxes as you need.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyXZwAVab8Cp"
   },
   "source": [
    "Use the trained model to make predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "EvMfVum6b_9b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write your code here. Add as many boxes as you need.\n",
    "predictions = trainer.predict(tokenized_dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VxAvDPtcNCh"
   },
   "source": [
    "Assess the performance of the model by using different metrics provided by the `scikit-learn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.70      0.68      1000\n",
      "           1       0.68      0.64      0.66      1000\n",
      "\n",
      "    accuracy                           0.67      2000\n",
      "   macro avg       0.67      0.67      0.67      2000\n",
      "weighted avg       0.67      0.67      0.67      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(predictions.predictions, axis=1)  # Convert logits to class predictions\n",
    "y_true = np.array(tokenized_dataset['test']['points'])\n",
    "\n",
    "# Generate the classification report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwnD_qSpIeXG"
   },
   "source": [
    "# Laboratory Exercise - Bonus Task (+ 2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At5xnEDq3A2h"
   },
   "source": [
    "Implement a simple machine learning pipeline to classify wine reviews as **exceptional** or not. Use TF-IDF vectorization to convert text into numerical features and train a logistic regression. Split the dataset into training and testing sets, fit the pipeline on the training data, and evaluate its performance using metrics such as precision, recall, and F1-score. Analyze the texts to find the most influential words or phrases associated with the **exceptional** wines. Use the coefficients from the logistic regression trained on TF-IDF features to identify the top positive and negative keywords for **exceptional** wines. Present these keywords in a simple table or visualization (e.g., bar chart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akxvW5SZ3DoV"
   },
   "outputs": [],
   "source": [
    "# Write your code here. Add as many boxes as you need."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
